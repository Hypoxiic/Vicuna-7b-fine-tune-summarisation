{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install transformers\n!pip install -q -U bitsandbytes\n!pip install -q -U git+https://github.com/huggingface/transformers.git\n!pip install -q -U git+https://github.com/huggingface/peft.git\n!pip install -q -U git+https://github.com/huggingface/accelerate.git","metadata":{"execution":{"iopub.status.busy":"2023-08-13T19:11:42.439814Z","iopub.execute_input":"2023-08-13T19:11:42.440590Z","iopub.status.idle":"2023-08-13T19:13:46.017650Z","shell.execute_reply.started":"2023-08-13T19:11:42.440548Z","shell.execute_reply":"2023-08-13T19:13:46.016309Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Requirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (4.30.2)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers) (3.12.2)\nRequirement already satisfied: huggingface-hub<1.0,>=0.14.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.16.4)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (1.23.5)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (6.0)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (2023.6.3)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers) (2.31.0)\nRequirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.13.3)\nRequirement already satisfied: safetensors>=0.3.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.3.1)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers) (4.65.0)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (2023.6.0)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (4.6.3)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->transformers) (3.0.9)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.1.0)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.4)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (1.26.15)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (2023.5.7)\n","output_type":"stream"}]},{"cell_type":"code","source":"import pandas as pd\nimport torch\nfrom transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\nfrom peft import prepare_model_for_kbit_training, LoraConfig, get_peft_model\nfrom sklearn.model_selection import train_test_split\nfrom torch.utils.data import DataLoader, TensorDataset\nfrom transformers import TrainingArguments, DataCollatorForLanguageModeling, Trainer","metadata":{"execution":{"iopub.status.busy":"2023-08-13T19:13:46.020058Z","iopub.execute_input":"2023-08-13T19:13:46.020422Z","iopub.status.idle":"2023-08-13T19:13:59.331004Z","shell.execute_reply.started":"2023-08-13T19:13:46.020384Z","shell.execute_reply":"2023-08-13T19:13:59.330037Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:98: UserWarning: unable to load libtensorflow_io_plugins.so: unable to open file: libtensorflow_io_plugins.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so']\ncaused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so: undefined symbol: _ZN3tsl6StatusC1EN10tensorflow5error4CodeESt17basic_string_viewIcSt11char_traitsIcEENS_14SourceLocationE']\n  warnings.warn(f\"unable to load libtensorflow_io_plugins.so: {e}\")\n/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:104: UserWarning: file system plugins are not loaded: unable to open file: libtensorflow_io.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so']\ncaused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so: undefined symbol: _ZTVN10tensorflow13GcsFileSystemE']\n  warnings.warn(f\"file system plugins are not loaded: {e}\")\n","output_type":"stream"}]},{"cell_type":"code","source":"# Define 4-bit quantization configuration\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_use_double_quant=True,\n    bnb_4bit_quant_type=\"nf4\",\n    bnb_4bit_compute_dtype=torch.bfloat16\n)","metadata":{"execution":{"iopub.status.busy":"2023-08-13T19:13:59.332587Z","iopub.execute_input":"2023-08-13T19:13:59.333395Z","iopub.status.idle":"2023-08-13T19:13:59.339665Z","shell.execute_reply.started":"2023-08-13T19:13:59.333357Z","shell.execute_reply":"2023-08-13T19:13:59.338754Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"#model_id = \"lmsys/vicuna-7b-v1.3\"","metadata":{"execution":{"iopub.status.busy":"2023-08-13T19:13:59.342565Z","iopub.execute_input":"2023-08-13T19:13:59.343254Z","iopub.status.idle":"2023-08-13T19:13:59.351979Z","shell.execute_reply.started":"2023-08-13T19:13:59.343207Z","shell.execute_reply":"2023-08-13T19:13:59.350882Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"# Load the pre-trained causal language model with quantization\nmodel_id = \"lmsys/vicuna-7b-v1.3\"\nmodel = AutoModelForCausalLM.from_pretrained(model_id, quantization_config=bnb_config)\ntokenizer = AutoTokenizer.from_pretrained(model_id)","metadata":{"execution":{"iopub.status.busy":"2023-08-13T19:13:59.353205Z","iopub.execute_input":"2023-08-13T19:13:59.353664Z","iopub.status.idle":"2023-08-13T19:16:43.753677Z","shell.execute_reply.started":"2023-08-13T19:13:59.353630Z","shell.execute_reply":"2023-08-13T19:16:43.752590Z"},"trusted":true},"execution_count":5,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading (…)lve/main/config.json:   0%|          | 0.00/566 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"207296187f4b4001a194d91a5e34432f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)model.bin.index.json:   0%|          | 0.00/26.8k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"23966005fecd4ef99bf95fdd9e9723b7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"827a15f331b64d80890ad28f9d72d711"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)l-00001-of-00002.bin:   0%|          | 0.00/9.98G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a995de1d9a0246e5887b598e46434ae5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)l-00002-of-00002.bin:   0%|          | 0.00/3.50G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2b46cc9c3c1247e69142e443afb97c07"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2ade185136b543368384a899addc7c40"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)neration_config.json:   0%|          | 0.00/132 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"942f951da601434694da7122035fef7d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)okenizer_config.json:   0%|          | 0.00/727 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"768a74bfeb4d4fc38c78224d33f57652"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"23e8f97857e74b8a9d786e983a4a5b42"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)cial_tokens_map.json:   0%|          | 0.00/435 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6cf92751e145459b8106763eb13a0d92"}},"metadata":{}},{"name":"stderr","text":"You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This means that tokens that come after special tokens will not be properly handled. We recommend you to read the related pull request available at https://github.com/huggingface/transformers/pull/24565, and set the legacy attribute accordingly.\n","output_type":"stream"}]},{"cell_type":"code","source":"# Enable Gradient Checkpointing and Prepare for k-bit Training\nmodel.gradient_checkpointing_enable()\nmodel = prepare_model_for_kbit_training(model)\n","metadata":{"execution":{"iopub.status.busy":"2023-08-13T19:16:43.755033Z","iopub.execute_input":"2023-08-13T19:16:43.755375Z","iopub.status.idle":"2023-08-13T19:16:43.774389Z","shell.execute_reply.started":"2023-08-13T19:16:43.755342Z","shell.execute_reply":"2023-08-13T19:16:43.773350Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"#model","metadata":{"execution":{"iopub.status.busy":"2023-08-13T19:16:43.775771Z","iopub.execute_input":"2023-08-13T19:16:43.776096Z","iopub.status.idle":"2023-08-13T19:16:43.780306Z","shell.execute_reply.started":"2023-08-13T19:16:43.776065Z","shell.execute_reply":"2023-08-13T19:16:43.779402Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"# Apply LoRA Configuration\nconfig = LoraConfig(\n    r=8, \n    lora_alpha=32, \n    target_modules=[\"q_proj\",\"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"], \n    lora_dropout=0.05, \n    bias=\"none\", \n    task_type=\"CAUSAL_LM\"\n)\nmodel = get_peft_model(model, config)","metadata":{"execution":{"iopub.status.busy":"2023-08-13T19:16:43.781747Z","iopub.execute_input":"2023-08-13T19:16:43.782297Z","iopub.status.idle":"2023-08-13T19:17:41.002325Z","shell.execute_reply.started":"2023-08-13T19:16:43.782265Z","shell.execute_reply":"2023-08-13T19:17:41.001256Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"#!nvidia-smi","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2023-08-13T19:17:41.004002Z","iopub.execute_input":"2023-08-13T19:17:41.004379Z","iopub.status.idle":"2023-08-13T19:17:41.009590Z","shell.execute_reply.started":"2023-08-13T19:17:41.004342Z","shell.execute_reply":"2023-08-13T19:17:41.008528Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"# Load and preprocess data\nfile_path = '/kaggle/input/wikipedia-summary-subset1k/df_subset_50000_with_prompt_filtered.csv'\n# df = pd.read_csv(file_path)\n# tokenizer = AutoTokenizer.from_pretrained(model_id)\n# tokenized_prompts = tokenizer.batch_encode_plus(df['prompt'].tolist(), truncation=True, padding=False)\n# prompt_lengths = [len(tokens) for tokens in tokenized_prompts['input_ids']]\ndf = pd.read_csv(file_path)\nfiltered_df = df[:1000]","metadata":{"execution":{"iopub.status.busy":"2023-08-13T19:30:31.067358Z","iopub.execute_input":"2023-08-13T19:30:31.067995Z","iopub.status.idle":"2023-08-13T19:30:31.603135Z","shell.execute_reply.started":"2023-08-13T19:30:31.067949Z","shell.execute_reply":"2023-08-13T19:30:31.602078Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"len(filtered_df)","metadata":{"execution":{"iopub.status.busy":"2023-08-13T19:30:33.861832Z","iopub.execute_input":"2023-08-13T19:30:33.862269Z","iopub.status.idle":"2023-08-13T19:30:33.873211Z","shell.execute_reply.started":"2023-08-13T19:30:33.862233Z","shell.execute_reply":"2023-08-13T19:30:33.872203Z"},"trusted":true},"execution_count":12,"outputs":[{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"1000"},"metadata":{}}]},{"cell_type":"code","source":"#DO ENTIRE DATASET AND LIMIT TO A 10000 FIRST OR 1K FIRST. ","metadata":{"execution":{"iopub.status.busy":"2023-08-13T19:17:41.031673Z","iopub.status.idle":"2023-08-13T19:17:41.032189Z","shell.execute_reply.started":"2023-08-13T19:17:41.031943Z","shell.execute_reply":"2023-08-13T19:17:41.031967Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Tokenize the prompts and create the targets\ninput_data = tokenizer.batch_encode_plus(filtered_df['prompt'].tolist(), truncation=True, padding=True, return_tensors='pt')\ninput_ids = input_data['input_ids']\nattention_mask = input_data['attention_mask']\nlabels = input_ids[:, 1:].clone()\nlabels[labels == tokenizer.pad_token_id] = -100\n\n# Create a TensorDataset for training\ntrain_dataset = TensorDataset(input_ids[:, :-1], attention_mask[:, :-1], labels)\n\n# Define a custom data collator\nclass CustomDataCollator:\n    def __call__(self, batch):\n        input_ids, attention_mask, labels = zip(*batch)\n        return {\n            \"input_ids\": torch.stack(input_ids),\n            \"attention_mask\": torch.stack(attention_mask),\n            \"labels\": torch.stack(labels),\n        }\n\n# Training arguments\ntraining_args = TrainingArguments(\n    per_device_train_batch_size=4, # could do 4 \n    gradient_accumulation_steps=8, # could do 8, giving an effective batch size of 32. Do max of 16. Keep increasing train batch size until memory runs out\n    num_train_epochs=5, # Train for 5 epochs\n    learning_rate=2e-4,\n    fp16=True,\n    logging_steps=1,\n    save_total_limit=3, # Limit the number of checkpoint files to keep\n    save_steps=250, # Save a checkpoint every 250 steps\n    output_dir=\"/kaggle/working/outputs\",\n    report_to=[] # Disable wandb integration if needed\n)\n\n# Create the Trainer\ntrainer = Trainer(\n    model=model,\n    train_dataset=train_dataset,\n    args=training_args,\n    data_collator=CustomDataCollator(),\n)\n\n# Set use_cache to False\nmodel.config.use_cache = False\n\n# Start training\ntrainer.train()\n","metadata":{"execution":{"iopub.status.busy":"2023-08-13T19:30:36.077991Z","iopub.execute_input":"2023-08-13T19:30:36.078391Z","iopub.status.idle":"2023-08-14T04:32:57.938785Z","shell.execute_reply.started":"2023-08-13T19:30:36.078357Z","shell.execute_reply":"2023-08-14T04:32:57.937763Z"},"trusted":true},"execution_count":13,"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='155' max='155' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [155/155 8:58:47, Epoch 4/5]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>13.399800</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>12.176900</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>11.198300</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>9.555400</td>\n    </tr>\n    <tr>\n      <td>5</td>\n      <td>8.528200</td>\n    </tr>\n    <tr>\n      <td>6</td>\n      <td>7.384700</td>\n    </tr>\n    <tr>\n      <td>7</td>\n      <td>6.698500</td>\n    </tr>\n    <tr>\n      <td>8</td>\n      <td>6.465000</td>\n    </tr>\n    <tr>\n      <td>9</td>\n      <td>6.296300</td>\n    </tr>\n    <tr>\n      <td>10</td>\n      <td>6.064100</td>\n    </tr>\n    <tr>\n      <td>11</td>\n      <td>5.739300</td>\n    </tr>\n    <tr>\n      <td>12</td>\n      <td>5.519400</td>\n    </tr>\n    <tr>\n      <td>13</td>\n      <td>5.322200</td>\n    </tr>\n    <tr>\n      <td>14</td>\n      <td>5.150800</td>\n    </tr>\n    <tr>\n      <td>15</td>\n      <td>4.790300</td>\n    </tr>\n    <tr>\n      <td>16</td>\n      <td>4.817600</td>\n    </tr>\n    <tr>\n      <td>17</td>\n      <td>4.582300</td>\n    </tr>\n    <tr>\n      <td>18</td>\n      <td>4.564700</td>\n    </tr>\n    <tr>\n      <td>19</td>\n      <td>4.424500</td>\n    </tr>\n    <tr>\n      <td>20</td>\n      <td>4.221900</td>\n    </tr>\n    <tr>\n      <td>21</td>\n      <td>4.056200</td>\n    </tr>\n    <tr>\n      <td>22</td>\n      <td>3.873200</td>\n    </tr>\n    <tr>\n      <td>23</td>\n      <td>3.971600</td>\n    </tr>\n    <tr>\n      <td>24</td>\n      <td>4.000800</td>\n    </tr>\n    <tr>\n      <td>25</td>\n      <td>3.911100</td>\n    </tr>\n    <tr>\n      <td>26</td>\n      <td>3.820100</td>\n    </tr>\n    <tr>\n      <td>27</td>\n      <td>3.618300</td>\n    </tr>\n    <tr>\n      <td>28</td>\n      <td>3.887600</td>\n    </tr>\n    <tr>\n      <td>29</td>\n      <td>3.527300</td>\n    </tr>\n    <tr>\n      <td>30</td>\n      <td>3.554200</td>\n    </tr>\n    <tr>\n      <td>31</td>\n      <td>3.501300</td>\n    </tr>\n    <tr>\n      <td>32</td>\n      <td>3.453800</td>\n    </tr>\n    <tr>\n      <td>33</td>\n      <td>3.389000</td>\n    </tr>\n    <tr>\n      <td>34</td>\n      <td>3.186400</td>\n    </tr>\n    <tr>\n      <td>35</td>\n      <td>3.320200</td>\n    </tr>\n    <tr>\n      <td>36</td>\n      <td>3.208000</td>\n    </tr>\n    <tr>\n      <td>37</td>\n      <td>3.525900</td>\n    </tr>\n    <tr>\n      <td>38</td>\n      <td>3.334800</td>\n    </tr>\n    <tr>\n      <td>39</td>\n      <td>3.271700</td>\n    </tr>\n    <tr>\n      <td>40</td>\n      <td>3.222300</td>\n    </tr>\n    <tr>\n      <td>41</td>\n      <td>2.972800</td>\n    </tr>\n    <tr>\n      <td>42</td>\n      <td>3.336500</td>\n    </tr>\n    <tr>\n      <td>43</td>\n      <td>3.266700</td>\n    </tr>\n    <tr>\n      <td>44</td>\n      <td>3.052000</td>\n    </tr>\n    <tr>\n      <td>45</td>\n      <td>3.113700</td>\n    </tr>\n    <tr>\n      <td>46</td>\n      <td>3.218400</td>\n    </tr>\n    <tr>\n      <td>47</td>\n      <td>3.227700</td>\n    </tr>\n    <tr>\n      <td>48</td>\n      <td>2.983300</td>\n    </tr>\n    <tr>\n      <td>49</td>\n      <td>3.194200</td>\n    </tr>\n    <tr>\n      <td>50</td>\n      <td>2.981300</td>\n    </tr>\n    <tr>\n      <td>51</td>\n      <td>3.225400</td>\n    </tr>\n    <tr>\n      <td>52</td>\n      <td>3.038300</td>\n    </tr>\n    <tr>\n      <td>53</td>\n      <td>3.039800</td>\n    </tr>\n    <tr>\n      <td>54</td>\n      <td>3.086900</td>\n    </tr>\n    <tr>\n      <td>55</td>\n      <td>2.926700</td>\n    </tr>\n    <tr>\n      <td>56</td>\n      <td>3.135400</td>\n    </tr>\n    <tr>\n      <td>57</td>\n      <td>2.975800</td>\n    </tr>\n    <tr>\n      <td>58</td>\n      <td>2.824200</td>\n    </tr>\n    <tr>\n      <td>59</td>\n      <td>2.911300</td>\n    </tr>\n    <tr>\n      <td>60</td>\n      <td>2.741500</td>\n    </tr>\n    <tr>\n      <td>61</td>\n      <td>2.733300</td>\n    </tr>\n    <tr>\n      <td>62</td>\n      <td>3.021400</td>\n    </tr>\n    <tr>\n      <td>63</td>\n      <td>2.747100</td>\n    </tr>\n    <tr>\n      <td>64</td>\n      <td>2.741400</td>\n    </tr>\n    <tr>\n      <td>65</td>\n      <td>2.882900</td>\n    </tr>\n    <tr>\n      <td>66</td>\n      <td>2.804700</td>\n    </tr>\n    <tr>\n      <td>67</td>\n      <td>2.664600</td>\n    </tr>\n    <tr>\n      <td>68</td>\n      <td>2.554800</td>\n    </tr>\n    <tr>\n      <td>69</td>\n      <td>2.715400</td>\n    </tr>\n    <tr>\n      <td>70</td>\n      <td>2.798900</td>\n    </tr>\n    <tr>\n      <td>71</td>\n      <td>2.527200</td>\n    </tr>\n    <tr>\n      <td>72</td>\n      <td>2.961000</td>\n    </tr>\n    <tr>\n      <td>73</td>\n      <td>2.895900</td>\n    </tr>\n    <tr>\n      <td>74</td>\n      <td>2.898800</td>\n    </tr>\n    <tr>\n      <td>75</td>\n      <td>2.611600</td>\n    </tr>\n    <tr>\n      <td>76</td>\n      <td>2.944600</td>\n    </tr>\n    <tr>\n      <td>77</td>\n      <td>2.716500</td>\n    </tr>\n    <tr>\n      <td>78</td>\n      <td>2.639600</td>\n    </tr>\n    <tr>\n      <td>79</td>\n      <td>2.756100</td>\n    </tr>\n    <tr>\n      <td>80</td>\n      <td>2.764200</td>\n    </tr>\n    <tr>\n      <td>81</td>\n      <td>2.774700</td>\n    </tr>\n    <tr>\n      <td>82</td>\n      <td>2.738900</td>\n    </tr>\n    <tr>\n      <td>83</td>\n      <td>2.771200</td>\n    </tr>\n    <tr>\n      <td>84</td>\n      <td>2.893000</td>\n    </tr>\n    <tr>\n      <td>85</td>\n      <td>2.662400</td>\n    </tr>\n    <tr>\n      <td>86</td>\n      <td>2.607100</td>\n    </tr>\n    <tr>\n      <td>87</td>\n      <td>2.562400</td>\n    </tr>\n    <tr>\n      <td>88</td>\n      <td>2.771900</td>\n    </tr>\n    <tr>\n      <td>89</td>\n      <td>2.772500</td>\n    </tr>\n    <tr>\n      <td>90</td>\n      <td>2.775200</td>\n    </tr>\n    <tr>\n      <td>91</td>\n      <td>2.468200</td>\n    </tr>\n    <tr>\n      <td>92</td>\n      <td>2.571300</td>\n    </tr>\n    <tr>\n      <td>93</td>\n      <td>2.774800</td>\n    </tr>\n    <tr>\n      <td>94</td>\n      <td>2.655100</td>\n    </tr>\n    <tr>\n      <td>95</td>\n      <td>2.562700</td>\n    </tr>\n    <tr>\n      <td>96</td>\n      <td>2.668200</td>\n    </tr>\n    <tr>\n      <td>97</td>\n      <td>2.448300</td>\n    </tr>\n    <tr>\n      <td>98</td>\n      <td>2.640100</td>\n    </tr>\n    <tr>\n      <td>99</td>\n      <td>2.630200</td>\n    </tr>\n    <tr>\n      <td>100</td>\n      <td>2.377800</td>\n    </tr>\n    <tr>\n      <td>101</td>\n      <td>2.780000</td>\n    </tr>\n    <tr>\n      <td>102</td>\n      <td>2.707000</td>\n    </tr>\n    <tr>\n      <td>103</td>\n      <td>2.642300</td>\n    </tr>\n    <tr>\n      <td>104</td>\n      <td>2.593200</td>\n    </tr>\n    <tr>\n      <td>105</td>\n      <td>2.690000</td>\n    </tr>\n    <tr>\n      <td>106</td>\n      <td>2.442500</td>\n    </tr>\n    <tr>\n      <td>107</td>\n      <td>2.570400</td>\n    </tr>\n    <tr>\n      <td>108</td>\n      <td>2.452800</td>\n    </tr>\n    <tr>\n      <td>109</td>\n      <td>2.567000</td>\n    </tr>\n    <tr>\n      <td>110</td>\n      <td>2.670900</td>\n    </tr>\n    <tr>\n      <td>111</td>\n      <td>2.630700</td>\n    </tr>\n    <tr>\n      <td>112</td>\n      <td>2.531300</td>\n    </tr>\n    <tr>\n      <td>113</td>\n      <td>2.839000</td>\n    </tr>\n    <tr>\n      <td>114</td>\n      <td>2.309300</td>\n    </tr>\n    <tr>\n      <td>115</td>\n      <td>2.601800</td>\n    </tr>\n    <tr>\n      <td>116</td>\n      <td>2.642800</td>\n    </tr>\n    <tr>\n      <td>117</td>\n      <td>2.606500</td>\n    </tr>\n    <tr>\n      <td>118</td>\n      <td>2.498400</td>\n    </tr>\n    <tr>\n      <td>119</td>\n      <td>2.669900</td>\n    </tr>\n    <tr>\n      <td>120</td>\n      <td>2.350800</td>\n    </tr>\n    <tr>\n      <td>121</td>\n      <td>2.615900</td>\n    </tr>\n    <tr>\n      <td>122</td>\n      <td>2.655700</td>\n    </tr>\n    <tr>\n      <td>123</td>\n      <td>2.360500</td>\n    </tr>\n    <tr>\n      <td>124</td>\n      <td>2.778000</td>\n    </tr>\n    <tr>\n      <td>125</td>\n      <td>2.524300</td>\n    </tr>\n    <tr>\n      <td>126</td>\n      <td>2.535200</td>\n    </tr>\n    <tr>\n      <td>127</td>\n      <td>2.310100</td>\n    </tr>\n    <tr>\n      <td>128</td>\n      <td>2.583600</td>\n    </tr>\n    <tr>\n      <td>129</td>\n      <td>2.431300</td>\n    </tr>\n    <tr>\n      <td>130</td>\n      <td>2.729100</td>\n    </tr>\n    <tr>\n      <td>131</td>\n      <td>2.463300</td>\n    </tr>\n    <tr>\n      <td>132</td>\n      <td>2.516600</td>\n    </tr>\n    <tr>\n      <td>133</td>\n      <td>2.396500</td>\n    </tr>\n    <tr>\n      <td>134</td>\n      <td>2.360500</td>\n    </tr>\n    <tr>\n      <td>135</td>\n      <td>2.471800</td>\n    </tr>\n    <tr>\n      <td>136</td>\n      <td>2.445800</td>\n    </tr>\n    <tr>\n      <td>137</td>\n      <td>2.533600</td>\n    </tr>\n    <tr>\n      <td>138</td>\n      <td>2.609100</td>\n    </tr>\n    <tr>\n      <td>139</td>\n      <td>2.599400</td>\n    </tr>\n    <tr>\n      <td>140</td>\n      <td>2.549900</td>\n    </tr>\n    <tr>\n      <td>141</td>\n      <td>2.622100</td>\n    </tr>\n    <tr>\n      <td>142</td>\n      <td>2.244200</td>\n    </tr>\n    <tr>\n      <td>143</td>\n      <td>2.616500</td>\n    </tr>\n    <tr>\n      <td>144</td>\n      <td>2.394300</td>\n    </tr>\n    <tr>\n      <td>145</td>\n      <td>2.509000</td>\n    </tr>\n    <tr>\n      <td>146</td>\n      <td>2.479300</td>\n    </tr>\n    <tr>\n      <td>147</td>\n      <td>2.556600</td>\n    </tr>\n    <tr>\n      <td>148</td>\n      <td>2.490200</td>\n    </tr>\n    <tr>\n      <td>149</td>\n      <td>2.662800</td>\n    </tr>\n    <tr>\n      <td>150</td>\n      <td>2.328200</td>\n    </tr>\n    <tr>\n      <td>151</td>\n      <td>2.570500</td>\n    </tr>\n    <tr>\n      <td>152</td>\n      <td>2.434900</td>\n    </tr>\n    <tr>\n      <td>153</td>\n      <td>2.695800</td>\n    </tr>\n    <tr>\n      <td>154</td>\n      <td>2.403800</td>\n    </tr>\n    <tr>\n      <td>155</td>\n      <td>2.527700</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=155, training_loss=3.3425499685349003, metrics={'train_runtime': 32538.4603, 'train_samples_per_second': 0.154, 'train_steps_per_second': 0.005, 'total_flos': 1.0318637137526784e+17, 'train_loss': 3.3425499685349003, 'epoch': 4.96})"},"metadata":{}}]},{"cell_type":"code","source":"import shutil\n\n# Save the model\nsave_path = \"/kaggle/working/my_model\"\nmodel.save_pretrained(save_path)\n\n# Save the tokenizer\ntokenizer.save_pretrained(save_path)\n\n# Compress the saved model directory\nshutil.make_archive(\"/kaggle/working/my_model.zip\", 'zip', save_path)\n\nprint(\"Model saved and compressed.\")\n","metadata":{"execution":{"iopub.status.busy":"2023-08-14T04:33:19.196503Z","iopub.execute_input":"2023-08-14T04:33:19.197284Z","iopub.status.idle":"2023-08-14T04:33:24.125116Z","shell.execute_reply.started":"2023-08-14T04:33:19.197240Z","shell.execute_reply":"2023-08-14T04:33:24.124004Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stdout","text":"Model saved and compressed.\n","output_type":"stream"}]}]}